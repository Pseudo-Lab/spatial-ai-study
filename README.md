<h1 align="center"> 스페셜한 Spatial AI 스터디 </h1>

<div align="center">
<a href="https://pseudo-lab.com"><img src="https://img.shields.io/badge/PseudoLab-S10-3776AB" alt="PseudoLab"/></a>
<a href="https://discord.gg/EPurkHVtp2"><img src="https://img.shields.io/badge/Discord-BF40BF" alt="Discord Community"/></a>
<a href="https://github.com/Pseudo-Lab/10th-template/stargazers"><img src="https://img.shields.io/github/stars/Pseudo-Lab/spatial-ai-study" alt="Stars Badge"/></a>
<a href="https://github.com/Pseudo-Lab/10th-template/network/members"><img src="https://img.shields.io/github/forks/Pseudo-Lab/spatial-ai-study" alt="Forks Badge"/></a>
<a href="https://github.com/Pseudo-Lab/10th-template/pulls"><img src="https://img.shields.io/github/issues-pr/Pseudo-Lab/spatial-ai-study" alt="Pull Requests Badge"/></a>
<a href="https://github.com/Pseudo-Lab/10th-template/issues"><img src="https://img.shields.io/github/issues/Pseudo-Lab/spatial-ai-study" alt="Issues Badge"/></a>
<a href="https://github.com/Pseudo-Lab/10th-template/graphs/contributors"><img alt="GitHub contributors" src="https://img.shields.io/github/contributors/Pseudo-Lab/spatial-ai-study?color=2b9348"></a>
<a href="https://hits.seeyoufarm.com"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https://github.com/Pseudo-Lab/spatial-ai-study&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false"/></a>
</div>
<br>

<!-- sheilds: https://shields.io/ -->
<!-- hits badge: https://hits.seeyoufarm.com/ -->

> '스페셜한 Spatial AI' 스터디 모임에 오신 것을 환영합니다! 

## 🌟 프로젝트 목표 (Project Vision)

Embodied AI에 대해 아시나요?
로보틱스 기술의 세부 분야로서, 컴퓨터 속에서만 존재하는 AI를 실제 세상과 물리적으로 상호작용할 수 있는 '몸을 가진 AI'를 만들기 위한 연구 분야입니다.
이러한 Embodied AI를 만들기 위한 기술 중 하나로 '공간 지능' - Spatial AI가 떠오르고 있습니다.

근데 이 기술, 범위가 너무 넓습니다.
우리는 여러 기수동안 유명한 교수님들의 주요 논문을 읽으며 Spatial AI에 대한 이해를 높일겁니다.

본인이 가장 익숙한 / 공부하고 싶은 논문을 골라서, 매주 차례를 돌며 논문 세미나를 진행합니다.

- 9기: Andrew Davison 교수님의 [Gaussian Belief Propagation](https://github.com/Pseudo-Lab/spatial-ai-study/edit/main/README.md#gaussian-belief-propagation-s9), [Coded SLAM with learned prior](https://github.com/Pseudo-Lab/spatial-ai-study/edit/main/README.md#coded-slam-with-learned-priors-s9) 이해하기
- 10기: [Object/Semantic SLAM]()
- 미래: SLAM with Scene graph, Visual-language topological mapping, NeRF와 3DGS를 이용한 Neural implicit feature fusion, Parallel proximity programming 공부하기!

## 🧑 역동적인 팀 소개 (Dynamic Team)

| 역할          | 이름 |  기술 스택 배지                                                                 | 주요 관심 분야                          |
|---------------|------|-----------------------------------------------------------------------|----------------------------------------|
| **Project Manager** | 장형기 | ![Python](https://img.shields.io/badge/Python-Expert-3776AB) ![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C) | 생성형 AI/추천 시스템 최적화             |
| **Member** | 레오나르도 다빈치 | ![SQL](https://img.shields.io/badge/SQL-Advanced-003B57) ![BigQuery](https://img.shields.io/badge/BigQuery-4285F4) | 데이터 파이프라인 설계                  |


## 💻 주차별 활동 (Activity plan)

매주 2명의 인원이 각각 1개의 논문에 해당하는 30분 세미나를 준비해 발표합니다.

- 시간: 화요일 오후 9시
- 장소: [#ROOM-YB](https://discord.gg/EPurkHVtp2)

| 날짜 | 내용 | 발표자 | 매체 | 
| ----- | ----- | -------- | ----- |
| 2025/02/XX | [3D Scene Graph: A Structure for Unified Semantics, 3D Space, and Camera](https://arxiv.org/abs/1910.02527), [3D Dynamic Scene Graphs: Actionable Spatial Perception with Places, Objects, and Humans](https://arxiv.org/abs/2002.06289) | [발표자]() [발표자]() | [링크]() | 
| 2025/02/XX | [Kimera: an open-source library for real-time metric-semantic localization and mapping](https://arxiv.org/abs/1910.02490), [Kimera: From SLAM to spatial perception with 3D dynamic scene graphs](https://arxiv.org/abs/2101.06894) | [발표자]() [발표자]() | [링크]() | 
| 2025/09/XX | [Kimera-multi: Robust, distributed, dense metric-semantic slam for multi-robot systems](https://arxiv.org/abs/2106.14386), [Hydra: A real-time spatial perception system for 3D scene graph construction and optimization](https://arxiv.org/abs/2201.13360) | [발표자]() [발표자]() | [링크]() | 
| 2025/10/XX | [Hierarchical representations and explicit memory: Learning effective navigation policies on 3d scene graphs using graph neural networks](https://arxiv.org/abs/2108.01176), [Hydra-multi: Collaborative online construction of 3d scene graphs with multi-robot teams](https://arxiv.org/abs/2304.13487) | [발표자]() [발표자]() | [링크]() |
| 2025/10/XX | [Kimera2: Robust and accurate metric-semantic SLAM in the real world](https://arxiv.org/abs/2401.06323), [Foundations of spatial perception for robotics: Hierarchical representations and real-time systems](https://arxiv.org/abs/2305.07154) | [발표자]() [발표자]() | [링크]() | 
| 2025/10/XX | [Khronos: A Unified Approach for Spatio-Temporal Metric-Semantic SLAM in Dynamic Environments](https://arxiv.org/abs/2402.13817), [Indoor and outdoor 3d scene graph generation via language-enabled spatial ontologies](https://arxiv.org/abs/2312.11713) | [발표자]() [발표자]() | [링크]() | 
| 2025/10/XX | [Task and Motion Planning in Hierarchical 3D Scene Graphs](https://arxiv.org/abs/2403.08094), [Clio: Real-time Task-Driven Open-Set 3D Scene Graphs](https://arxiv.org/abs/2404.13696) | [발표자]() [발표자]() | [링크]() | 
| 2025/10/XX | [Long-Term Human Trajectory Prediction using 3D Dynamic Scene Graphs](https://arxiv.org/abs/2404.13696), [S-Graphs+: Real-Time Localization and Mapping Leveraging Hierarchical Representations](https://arxiv.org/abs/2212.11770)| [발표자]() [발표자]() | [링크]() | 
| 2025/11/XX | [SceneGraphFusion: Incremental 3D Scene Graph Prediction from RGB-D Sequences](https://arxiv.org/abs/2103.14898), [ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning](https://arxiv.org/abs/2309.16650) | [발표자]() [발표자]() | [링크]() | 
| 2025/11/XX | [Collaborative Dynamic 3D Scene Graphs for Automated Driving](https://arxiv.org/abs/2309.06635), [SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning](https://arxiv.org/abs/2307.06135) | [발표자]() [발표자]() | [링크]() | 
| 2025/11/XX | [Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with Queryable Objects and Open-Set Relationships](https://arxiv.org/abs/2402.12259), [Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation](https://arxiv.org/abs/2403.17846) | [발표자]() [발표자]() | [링크]() | 
| 2025/11/XX | [LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action](https://arxiv.org/abs/2207.04429), [CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory](https://arxiv.org/abs/2210.05663) | [발표자]() [발표자]() | [링크]() | 
| 2025/12/XX | [Graph R-CNN for Scene Graph Generation](https://arxiv.org/abs/1808.00191)_ | [발표자]() [발표자]() | [링크]() | 
| 2025/12/XX |  | [발표자]() [발표자]() | [링크]() | 

## 과거의 활동 (Activity history)

### Gaussian Belief Propagation (S9)

<details>
  <summary> Click me to open the list </summary>
  
| 날짜 | 내용 | 발표자 | 매체 | 
| ----- | ----- | -------- | ----- |
| 2024/09/10 | OT (스터디 소개, 방향 논의, Ice breaking), 발표 논문 선정 | [장형기]() | - | 
| 2024/09/17 |  추석! 쉽니다!  | - | - | 
| 2024/09/24 | [FutureMapping: The Computational Structure of Spatial AI Systems](https://arxiv.org/abs/1803.11288), [FutureMapping 2: Gaussian Belief Propagation for Spatial AI](https://arxiv.org/abs/1910.14139) | [곽명철](https://github.com/thirdcat) [김선호](https://github.com/Philipshrimp) | [YouTube](https://youtu.be/JXyrsKVz8EY?si=2ThfW4Sl2AI970k2), [YouTube](https://youtu.be/UDzAVhhVmB0?si=ENCjuKHL13riizIs) | 
| 2024/10/01 | 임시공휴일! 쉽니다! | - | - |
| 2024/10/08 |[Bundle adjustment on a graph processor](https://arxiv.org/abs/2003.03134), [Visual introduction to Gaussian Belief Propagation](https://arxiv.org/abs/2107.02308) | [김희성]() [김지성](https://github.com/JeeseongKim) | [YouTube](https://youtu.be/H5pXj5iwPAY?si=hQXl5AMPFeplcOdf), [YouTube](https://youtu.be/22r9V92OIak?si=UZFlePdDIxDf0XPh) | 
| 2024/10/15 | [Incremental Abstraction in Distributed Probabilistic SLAM graphs](https://arxiv.org/abs/2109.06241), [Visual Odometry using a Focal-Plane sensor-processor](https://www.imperial.ac.uk/media/imperial-college/faculty-of-engineering/computing/public/1819-ug-projects/MuraiR-Visual-Odometry-Using-a-Focal-plane-Sensor-processor.pdf) | [박정현]() [이원희](https://github.com/Tershire) | [YouTube](https://youtu.be/n896HFsn7Ak?si=BVpZbDMimG_BLZI5), [YouTube](https://youtu.be/J4usWy91-0U?si=n1IfuNZgVQsxoawV) | 
| 2024/10/22 | [BIT-VO: Visual Odometry at 300 FPS using Binary Features from the Focal Plane](https://arxiv.org/abs/2004.11186), [A Robot web for distributed many-device localisation](https://arxiv.org/abs/2202.03314) | [윤혁진](https://github.com/yhjin1096) [장형기]() | [YouTube](https://youtu.be/p7WAS2VRe9Q?si=tH-mMVZW1UHmX1eb), [YouTube](https://youtu.be/dS9wz9Yn748?si=DwZSmHi80VHcUQMw) | 
| 2024/10/29 | 가짜연 행사로 1주 미룹니다! |  |  | 
| 2024/11/05 | [Learning in deep factor graphs with Gaussian Belief Propagation](https://arxiv.org/abs/2311.14649), [Distributed Simultaneous Localisation and Auto-Calibration using Gaussian Belief Propagation](https://arxiv.org/abs/2401.15036) | [김선호](https://github.com/Philipshrimp) [이재민]() | [YouTube](https://youtu.be/ZDt3c9qfPEw?si=H3kjO71lgBqbLsh6), [YouTube](https://youtu.be/1gZeMqx11Zk?si=gDHdnYRt25cuB5wB) | 
| 2024/11/12 | [PixRO: Pixel-Distributed Rotational Odometry with Gaussian Belief Propagation](https://arxiv.org/abs/2406.09726), | [이원희](https://github.com/Tershire) | [YouTube](https://youtu.be/ly8h0nT3j1w?si=6hD75xTW6drX3v5y) | 

</details>

### Coded SLAM with learned priors (S9)

<details>
  <summary> Click me to open the list </summary>
  
| 날짜 | 내용 | 발표자 | 매체 | 
| ----- | ----- | -------- | ----- |
| 2024/11/19 | [CodeSLAM - Learning a Compact, Optimisable Representation for Dense Visual SLAM](https://arxiv.org/abs/1804.00874), [SceneCode: Monocular Dense Semantic Reconstruction using Learned Encoded Scene Representations](https://arxiv.org/abs/1903.06482) | [박정현]() [곽명철](https://github.com/thirdcat)  | [YouTube](https://youtu.be/rQUNrk7-xlE?si=-tpIdUCgiLQP_pKQ), [YouTube](https://youtu.be/143wPBr-a2Y?si=w7BggylVj9OXavDG) | 
| 2024/11/26 | [DeepFactors: Real-Time Probabilistic Dense Monocular SLAM](https://arxiv.org/abs/2001.05049). [CodeMapping: Real-Time Dense Mapping for Sparse SLAM using Compact Scene Representations](https://arxiv.org/abs/2107.08994)  | [김희성]() [이재민]()  | [YouTube](https://youtu.be/vap7oZ5hg80?si=iQPTP08HW9H0RiYq), [YouTube](https://youtu.be/ySsNrE7Z1P8?si=PdCI34NCzZcT8rR0) | 
| 2024/12/03 |[End-to-End Egospheric spatial memory](https://arxiv.org/abs/2102.07764), [Learning Depth Covariance Function](https://arxiv.org/abs/2303.12157)  | [윤혁진](https://github.com/yhjin1096) [장형기]() | [YouTube](https://youtu.be/svbrkQs560Y?si=IT5qrzp_2giB581d), [YouTube](https://youtu.be/P8hI8IIzCew?si=D4ZM4Bd9ccTmYnUg) | 
| 2024/12/10 | [COMO: Compact Mapping and Odometry](https://arxiv.org/abs/2404.03531) | [김지성](https://github.com/JeeseongKim) | [YouTube](https://youtu.be/DZEA2P2KsK8?si=U2wk7dzdd3cQm7nV) | 

</details>


## 🌱 참여 안내 (How to Engage)
**팀원으로 참여하시려면 러너 모집 기간에 신청해주세요.**  
- 링크 (준비중)

**누구나 청강을 통해 모임을 참여하실 수 있습니다.**  
1. 특별한 신청 없이 정기 모임 시간에 맞추어 디스코드 #Room-AN 채널로 입장
2. Magical Week 중 행사에 참가
3. Pseudo Lab 행사에서 만나기

## 이전 기수 멤버들 (Alumni)



## Acknowledgement 🙏

'스페셜한 Spatial AI' is developed as part of Pseudo-Lab's Open Research Initiative. Special thanks to our contributors and the open source community for their valuable insights and contributions.

## About Pseudo Lab 👋🏼

[Pseudo-Lab](https://pseudo-lab.com/) is a non-profit organization focused on advancing machine learning and AI technologies. Our core values of Sharing, Motivation, and Collaborative Joy drive us to create impactful open-source projects. With over 5k+ researchers, we are committed to advancing machine learning and AI technologies.

## Alumnis 😃

<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
<table>
  <tr>
    <td align="center"><a href="https://github.com/Philipshrimp"><img src="https://avatars.githubusercontent.com/u/7986113?v=4" width="100px;" alt=""/><br /><sub><b>Sunho Kim </b></sub></a><br /><a href="https://github.com/Philipshrimp" title="GitHub"> :octocat:</a></td>
    <td align="center"><a href="https://github.com/Tershire"><img src="https://avatars.githubusercontent.com/u/84491744?v=4" width="100px;" alt=""/><br /><sub><b>Wonhee Lee </b></sub></a><br /><a href="https://github.com/Tershire" title="GitHub">:octocat:</a></td>
    <td align="center"><a href="https://github.com/JeeseongKim"><img src="https://avatars.githubusercontent.com/u/65888017?v=4" width="100px;" alt=""/><br /><sub><b>Jeeseong Kim </b></sub></a><br /><a href="https://github.com/JeeseongKim" title="GitHub">:octocat:</a></td>
    <td align="center"><a href="https://github.com/yhjin1096"><img src="https://avatars.githubusercontent.com/u/106298374?v=4" width="100px;" alt=""/><br /><sub><b>Hyeokjin Yoon </b></sub></a><br /><a href="https://github.com/yhjin1096" title="GitHub">:octocat:</a></td>
    <td align="center"><a href="https://github.com/thirdcat"><img src="https://avatars.githubusercontent.com/u/6870137?v=4" width="100px;" alt=""/><br /><sub><b>Myeongchul Kwak </b></sub></a><br /><a href="https://github.com/thirdcat" title="GitHub">:octocat:</a></td>
    <td align="center"><a href="https://github.com/luckydipper"><img src="https://avatars.githubusercontent.com/u/65158138?v=4" width="100px;" alt=""/><br /><sub><b>Heesung Kim </b></sub></a><br /><a href="https://github.com/luckydipper" title="GitHub">:octocat:</a></td>
    <td align="center"><a href="https://github.com/parkjh688"><img src="https://avatars.githubusercontent.com/u/36429399?v=4" width="100px;" alt=""/><br /><sub><b>Junghyun Park </b></sub></a><br /><a href="https://github.com/parkjh688" title="GitHub">:octocat:</a></td>
    <td align="center"><a href="https://github.com/elitechrome"><img src="https://avatars.githubusercontent.com/u/5326562?v=4" width="100px;" alt=""/><br /><sub><b>Jaemin Lee </b></sub></a><br /><a href="https://github.com/elitechrome" title="GitHub">:octocat:</a></td>
  </tr>
</table>

## License 🗞

This project is licensed under the [MIT License](https://opensource.org/licenses/MIT).
